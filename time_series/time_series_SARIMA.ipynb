{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "# might be add in requirements.txt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "# Statsmodel\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv(\"./train.csv\", parse_dates=[\"datetime\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the most important statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's graph the series during the last month. The train data only includes the first 19 days of the month\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df[\"count\"].values[-456:])\n",
    "plt.title(\"Amount of rented bike during the last month\")\n",
    "plt.ylabel(\"Rented Bikes\")\n",
    "plt.xlabel('Hours')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using some built-in functions, let's determine if the data is seasonal or not. \n",
    "# If the autocorrelation plot shows a sinusoidal shape, the data is seasonal (i.e., periodic)\n",
    "# We prefer seasonal data because it is easier to make predictions on it (using historical data). \n",
    "# Lags are previous timesteps\n",
    "\n",
    "plot_acf(df['count'].values)\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to have more information is using both the autocorrelation and the partial autocorrelation. \n",
    "# Additionally we can perform an hypothesis test to check if our data is stationary or not. \n",
    "# The lags are the number of previous time steps\n",
    "\n",
    "def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n",
    "    \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style='bmh'):\n",
    "        # Create figure and axes\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        \n",
    "        # Pot the variable\n",
    "        y.plot(ax=ts_ax)\n",
    "        # Returns the p_value of the Dickey-Fuller test. If p_value = 1, the process is not stationary\n",
    "        # If the process is stationary (p_value = 0), then the series has a constant mean.\n",
    "        p_value = adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        ts_ax.set_xlabel(\"Samples\")\n",
    "        plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        acf_ax.set_xlabel(\"Lag\")\n",
    "        plot_pacf(y, lags=lags, ax=pacf_ax, method=\"ywm\")\n",
    "        pacf_ax.set_xlabel(\"Lag\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "tsplot(df[\"count\"].values)\n",
    "\n",
    "# Notice that the ACF shows a sine-wave shape pattern, the PACF has spikes at lags 1 and 2, \n",
    "# and no correlation for other lags, which means a probable AR order of 2 and seasonal AR order of 2\n",
    "# For the PACF plot, initial spikes at lag = 1 and 2 and seasonal spikes at lag = 24, \n",
    "# which means a probable MA order of 2 and seasonal MA order of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's another graphical built-in function to see if a process is seasonal or not.\n",
    "def decompose_bike_sharing_demand(df, share_type='count', samples=250, period=24):\n",
    "    if samples == 'all':\n",
    "        res = seasonal_decompose(df[share_type].values, period=period)\n",
    "    else:\n",
    "        res = seasonal_decompose(df[share_type].values[-samples:], period=period)\n",
    "    \n",
    "    observed = res.observed\n",
    "    trend = res.trend\n",
    "    seasonal = res.seasonal\n",
    "    residual = res.resid\n",
    "\n",
    "    fig, axs = plt.subplots(4, figsize=(16,16))\n",
    "    axs[0].set_title('Observed', fontsize=16)\n",
    "    axs[0].plot(observed)\n",
    "    axs[0].grid()\n",
    "    \n",
    "    axs[1].set_title('Trend', fontsize=16)\n",
    "    axs[1].plot(trend)\n",
    "    axs[1].grid()\n",
    "\n",
    "    axs[2].set_title('Seasonality', fontsize=16)\n",
    "    axs[2].plot(seasonal)\n",
    "    axs[2].grid()\n",
    "\n",
    "    axs[3].set_title('Noise', fontsize=16)\n",
    "    axs[3].plot(residual)\n",
    "    axs[3].scatter(y=residual, x=range(len(residual)), alpha=0.5)\n",
    "    axs[3].grid()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# We will examine the last reported month in periods of 24 hours.\n",
    "decompose_bike_sharing_demand(df, samples=720, period=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we need to find a model that allow us to capture the singularities of this process \n",
    "# (e.g., mean, variance, seasonality, stationarity) so we are able to predict the future. \n",
    "\n",
    "# One of the easiest ways to model time series data is through the Moving Average, \n",
    "# which predicts the next value or observation is the mean of past values or observations (called window). \n",
    "\n",
    "# The longer the window, the smoother the variations (tendency to the mean), the smaller the window, \n",
    "# the most likely the model will follow the trend. \n",
    "\n",
    "def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n",
    "\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    \n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.plot(series[window:], label='Actual values')\n",
    "    plt.title('Moving average\\n window size = {}'.format(window))\n",
    "    plt.plot(rolling_mean, 'g', label='Rolling mean trend')\n",
    "    \n",
    "    #Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bound = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bound = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n",
    "        plt.plot(lower_bound, 'r--')\n",
    "            \n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth by the previous 1 days (24 hours)\n",
    "plot_moving_average(pd.Series(df[\"count\"].values), window=24, plot_intervals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA (Seasonal Autoregressive Integrated Moving Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SARIMA is the combination of the autoregressive part with the moving average. \n",
    "This is specially useful to time series that exhibit seasonality.\n",
    "\n",
    "As its name indicates, the first part is an autoregression (AR). \n",
    "Basically, we assume that an observation depends on its previous observations within some lag. \n",
    "Therefore, it exists strong correlation between the observations in a period of time. \n",
    "\n",
    "In the moving average (MA) part, we assume that there is strong correlation within the window. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "# include I if the data is non stationary\n",
    "sarima_model = SARIMAX(\n",
    "    pd.Series(df[\"count\"].values),\n",
    "    order=(2, 0, 2), # p=order[0], d=order[1], q=order[2], AR(p), I(d), MA(q)\n",
    "    seasonal_order=(2, 0, 2, 24), # Seasonality: P=seasonal_order[0], D=seasonal_order[1], Q=seasonal_order[2], s=seasonal_order[3]\n",
    ")\n",
    "trained_model = sarima_model.fit(disp=-1)\n",
    "print(trained_model.summary()) \n",
    "\n",
    "# Use the trained model to predict \n",
    "predictions = trained_model.predict()\n",
    "# Fix the predictions, we know that we cannot have -1 rented bike\n",
    "predictions[predictions < 0] =0\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(pd.Series(df['count'].values), predictions)}%\")\n",
    "\n",
    "# Some Results\n",
    "# SARIMAX(1, 0, 1)x(1, 0, 1, 24) --> MAPE: 0.5182085671253294%\n",
    "# SARIMAX(0, 1, 0) --> MAPE: 0.5896235403262953%\n",
    "# SARIMAX(0, 1, 0)x(0, 1, 0, 6)  --> MAPE: 0.8328176193518243%\n",
    "# SARIMAX(0, 1, 0)x(1, 0, 0, 12) --> MAPE: 0.6259597313147078%\n",
    "# SARIMAX(2, 0, 0)x(1, 0, 0, 24) --> MAPE: 0.4436969647510891%\n",
    "# SARIMAX(1, 0, 2)x(1, 0, [1], 24) --> MAPE: 0.5213048599649486%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.plot(pd.Series(df[\"count\"].values), label='Actual values')\n",
    "plt.plot(predictions, 'g', label='Predicted values')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set initial values and some bounds\n",
    "ps = range(0, 3)\n",
    "d = 0  # our data is stationary\n",
    "qs = range(0, 3)\n",
    "Ps = range(0, 3)\n",
    "D = 0  # our data is stationary\n",
    "Qs = range(0, 3)\n",
    "s = 24\n",
    "\n",
    "#Create a list with all possible combinations of parameters\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "print(len(parameters_list))\n",
    "\n",
    "\n",
    "def optimize_SARIMA(parameters_list, d, D, s):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        \n",
    "        parameters_list - list with (p, q, P, Q) tuples\n",
    "        d - integration order\n",
    "        D - seasonal integration order\n",
    "        s - length of season\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float('inf')\n",
    "    \n",
    "    for param in tqdm(parameters_list):\n",
    "        try: model = SARIMAX(pd.Series(df[\"count\"].values), order=(param[0], d, param[1]),\n",
    "                                               seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        aic = model.aic\n",
    "        \n",
    "        #Save best model, AIC and parameters\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "        \n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = ['parameters', 'aic']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results of the hyperparameter optimization\n",
    "result_table = optimize_SARIMA(parameters_list, d, D, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end the ultimate goal is to predict the last ten days of the total number of rented bikes having the first 20 days. \n",
    "# We will do it for the last month but the same can be done for the other months. \n",
    "\n",
    "# 19*24=456 --> We have an hourly report\n",
    "sarima_model = SARIMAX(\n",
    "    pd.Series(df[\"count\"].values[-456:]),\n",
    "    order=(2, 0, 2), # p=order[0], d=order[1], q=order[2], AR(p), I(d), MA(q)\n",
    "    seasonal_order=(2, 0, 2, 24), # Seasonality: P=seasonal_order[0], D=seasonal_order[1], Q=seasonal_order[2], s=seasonal_order[3]\n",
    ")\n",
    "# Don't display any warning during training. \n",
    "trained_model = sarima_model.fit(disp=-1)\n",
    "print(trained_model.summary()) \n",
    "\n",
    "# We can test the model using the training data, for example, trying to predict the last three days \n",
    "predictions = trained_model.predict(start=384, end=455, dynamic=True)\n",
    "# Fix the predictions, we know that we cannot have -1 rented bike\n",
    "predictions[predictions < 0] =0\n",
    "# print(f\"MAPE: {mean_absolute_percentage_error(pd.Series(df['count'].values[-456:]), predictions)}%\")\n",
    "\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.plot(pd.Series(df[\"count\"].values[-456:]), label='Actual values')\n",
    "plt.plot(predictions, 'g', label='Predicted values')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the model to predict the following ten days \n",
    "future_predictions = trained_model.predict(start=456, end=695, dynamic=True)\n",
    "future_predictions.describe()\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.plot(future_predictions, label='Forecasted values')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "# Generate future dates\n",
    "last_month_df = df.iloc[-456:, :]\n",
    "last_month_df[\"datetime\"] = pd.to_datetime(last_month_df[\"datetime\"])\n",
    "last_month_df.set_index(\"datetime\",inplace=True)\n",
    "last_month_df.index[-1]\n",
    "future_dates=[last_month_df.index[-1]+ DateOffset(hours=x)for x in range(0,241)]\n",
    "last_month_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframe\n",
    "future_dates_df=pd.DataFrame(index=future_dates[1:],columns=last_month_df.columns)\n",
    "future_dates_df['forecast'] = future_predictions.tolist()\n",
    "future_dates_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original and newly created dataset\n",
    "future_df=pd.concat([last_month_df,future_dates_df])\n",
    "future_df.tail()\n",
    "# Predict\n",
    "future_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.plot(pd.Series(future_df[\"count\"].values), label='Actual values')\n",
    "plt.plot(pd.Series(future_df[\"forecast\"].values), 'g', label='Predicted values')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5895fc9c045060c0483fdecda8d9b833399f927ce766293a7dd4fef0f0b97a32"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('iot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
